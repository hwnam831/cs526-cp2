//
// Generated by LLVM NVPTX Back-End
//

.version 5.0
.target sm_61, texmode_independent
.address_size 64

	// .globl	matmul                  // -- Begin function matmul
                                        // @matmul
.entry matmul(
	.param .u64 .ptr .global .align 4 matmul_param_0,
	.param .u64 .ptr .global .align 4 matmul_param_1,
	.param .u64 .ptr .global .align 4 matmul_param_2,
	.param .u32 matmul_param_3,
	.param .u32 matmul_param_4
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<22>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<22>;

// %bb.0:                               // %entry
	ld.param.u32 	%r14, [matmul_param_3];
	ld.param.u64 	%rd6, [matmul_param_2];
	setp.lt.s32 	%p1, %r14, 1;
	mov.u32 	%r1, %ctaid.y;
	shl.b32 	%r15, %r1, 4;
	mov.u32 	%r2, %tid.y;
	add.s32 	%r16, %r15, %r2;
	shl.b32 	%r3, %r16, 10;
	mov.u32 	%r17, %ctaid.x;
	shl.b32 	%r4, %r17, 4;
	mov.f32 	%f19, 0f00000000;
	mov.u32 	%r5, %tid.x;
	@%p1 bra 	LBB0_6;
// %bb.1:                               // %for.body.preheader
	ld.param.u64 	%rd5, [matmul_param_1];
	ld.param.u64 	%rd4, [matmul_param_0];
	and.b32  	%r6, %r14, 1;
	setp.eq.s32 	%p2, %r14, 1;
	mov.f32 	%f19, 0f00000000;
	mov.u32 	%r37, 0;
	@%p2 bra 	LBB0_4;
// %bb.2:                               // %for.body.preheader.new
	and.b32  	%r7, %r14, -2;
	shl.b32 	%r21, %r1, 14;
	shl.b32 	%r22, %r2, 10;
	add.s32 	%r8, %r21, %r22;
	mul.wide.u32 	%rd7, %r8, 4;
	add.s64 	%rd8, %rd7, %rd4;
	add.s64 	%rd21, %rd8, 4;
	mov.f32 	%f19, 0f00000000;
	mov.u32 	%r37, 0;
	mov.u32 	%r35, 1024;
LBB0_3:                                 // %for.body
                                        // =>This Inner Loop Header: Depth=1
	add.s32 	%r23, %r8, %r37;
	mul.wide.u32 	%rd9, %r23, 4;
	add.s64 	%rd10, %rd4, %rd9;
	ld.global.f32 	%f11, [%rd10];
	add.s32 	%r24, %r35, -1024;
	or.b32  	%r25, %r5, %r24;
	add.s32 	%r26, %r25, %r4;
	mul.wide.u32 	%rd11, %r26, 4;
	add.s64 	%rd12, %rd5, %rd11;
	ld.global.f32 	%f12, [%rd12];
	fma.rn.f32 	%f13, %f11, %f12, %f19;
	ld.global.f32 	%f14, [%rd21];
	or.b32  	%r27, %r5, %r35;
	add.s32 	%r28, %r27, %r4;
	mul.wide.u32 	%rd13, %r28, 4;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f32 	%f15, [%rd14];
	fma.rn.f32 	%f19, %f14, %f15, %f13;
	add.s32 	%r37, %r37, 2;
	add.s64 	%rd21, %rd21, 8;
	add.s32 	%r35, %r35, 2048;
	setp.ne.s32 	%p3, %r7, %r37;
	@%p3 bra 	LBB0_3;
LBB0_4:                                 // %for.end.loopexit.unr-lcssa
	setp.eq.s32 	%p4, %r6, 0;
	@%p4 bra 	LBB0_6;
// %bb.5:                               // %for.end.loopexit.epilog-lcssa
	add.s32 	%r29, %r3, %r37;
	mul.wide.u32 	%rd15, %r29, 4;
	add.s64 	%rd16, %rd4, %rd15;
	ld.global.f32 	%f16, [%rd16];
	shl.b32 	%r30, %r37, 10;
	or.b32  	%r31, %r5, %r30;
	add.s32 	%r32, %r31, %r4;
	mul.wide.u32 	%rd17, %r32, 4;
	add.s64 	%rd18, %rd5, %rd17;
	ld.global.f32 	%f17, [%rd18];
	fma.rn.f32 	%f19, %f16, %f17, %f19;
LBB0_6:                                 // %for.end
	add.s32 	%r33, %r4, %r5;
	add.s32 	%r34, %r33, %r3;
	mul.wide.u32 	%rd19, %r34, 4;
	add.s64 	%rd20, %rd6, %rd19;
	st.global.f32 	[%rd20], %f19;
	ret;
                                        // -- End function
}
